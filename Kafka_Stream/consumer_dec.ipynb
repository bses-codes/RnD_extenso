{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T04:37:17.586773Z",
     "start_time": "2024-05-14T04:37:17.520167Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import Normalizer, StandardScaler\n",
    "import random\n",
    "import base64\n",
    "from cryptography.fernet import Fernet\n",
    "\n",
    "kafka_topic_name = \"FC_Account_Master\"\n",
    "kafka_bootstrap_servers = 'localhost:9092'\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Structured Streaming \") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "df = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "        .option(\"subscribe\", kafka_topic_name) \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .load()\n",
    "\n",
    "df1 = df.selectExpr(\"CAST(value AS STRING)\", \"timestamp\")\n",
    "\n",
    "\n",
    "df_schema_string = \"order_id INT, encrypted_account_number STRING, product STRING, product_category STRING, encryption_key STRING\"\n",
    "\n",
    "\n",
    "def decrypt(encrypted_data, encryption_key):\n",
    "  fernet = Fernet(encryption_key.encode('utf-8'))\n",
    "  decrypted_data = fernet.decrypt(base64.b64decode(encrypted_data)).decode('utf-8')\n",
    "  return decrypted_data\n",
    "\n",
    "decrypt_udf = udf(decrypt, StringType())\n",
    "\n",
    "\n",
    "def extract_key(message):\n",
    "  return message.split(\",\")[-1]\n",
    "\n",
    "extract_key_udf = udf(extract_key, StringType())\n",
    "\n",
    "df2 = df1.select(from_csv(col(\"value\"), df_schema_string).alias(\"data\"), \"timestamp\")\n",
    "df2.printSchema()\n",
    "\n",
    "df3 = df2.withColumn(\"encryption_key\", extract_key_udf(col(\"encryption_key\")))\n",
    "df3 = df3.withColumn(\"account_number\", decrypt_udf(\"encrypted_account_number\", \"encryption_key\"))\n",
    "df3 = df3.select(\"data.*\", \"timestamp\")\n",
    "\n",
    "df3.createOrReplaceTempView(\"data_find\");\n",
    "song_find_text = spark.sql(\"SELECT * FROM data_find\")\n",
    "data_agg_write_stream = song_find_text \\\n",
    "        .writeStream \\\n",
    "        .trigger(processingTime='5 seconds') \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .option(\"truncate\", \"false\") \\\n",
    "        .format(\"memory\") \\\n",
    "        .queryName(\"testedTable\") \\\n",
    "        .start()\n",
    "\n",
    "data_agg_write_stream.awaitTermination(1)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- order_id: integer (nullable = true)\n",
      " |    |-- encrypted_account_number: string (nullable = true)\n",
      " |    |-- product: string (nullable = true)\n",
      " |    |-- product_category: string (nullable = true)\n",
      " |    |-- encryption_key: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `encryption_key` cannot be resolved. Did you mean one of the following? [`data`, `timestamp`].;\n'Project [data#256, timestamp#244, extract_key('encryption_key)#259 AS encryption_key#260]\n+- Project [from_csv(StructField(order_id,IntegerType,true), StructField(encrypted_account_number,StringType,true), StructField(product,StringType,true), StructField(product_category,StringType,true), StructField(encryption_key,StringType,true), value#253, Some(Asia/Kathmandu), None) AS data#256, timestamp#244]\n   +- Project [cast(value#240 as string) AS value#253, timestamp#244]\n      +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@32028d47, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7ba8ed23, [kafka.bootstrap.servers=localhost:9092, startingOffsets=earliest, subscribe=FC_Account_Master], [key#239, value#240, topic#241, partition#242, offset#243L, timestamp#244, timestampType#245], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@aeb221,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> FC_Account_Master, startingOffsets -> earliest),None), kafka, [key#232, value#233, topic#234, partition#235, offset#236L, timestamp#237, timestampType#238]\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 52\u001B[0m\n\u001B[1;32m     49\u001B[0m df2 \u001B[38;5;241m=\u001B[39m df1\u001B[38;5;241m.\u001B[39mselect(from_csv(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalue\u001B[39m\u001B[38;5;124m\"\u001B[39m), df_schema_string)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimestamp\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     50\u001B[0m df2\u001B[38;5;241m.\u001B[39mprintSchema()\n\u001B[0;32m---> 52\u001B[0m df3 \u001B[38;5;241m=\u001B[39m \u001B[43mdf2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencryption_key\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextract_key_udf\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcol\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencryption_key\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     53\u001B[0m df3 \u001B[38;5;241m=\u001B[39m df3\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccount_number\u001B[39m\u001B[38;5;124m\"\u001B[39m, decrypt_udf(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencrypted_account_number\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencryption_key\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m     54\u001B[0m df3 \u001B[38;5;241m=\u001B[39m df3\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata.*\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimestamp\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/sql/dataframe.py:5174\u001B[0m, in \u001B[0;36mDataFrame.withColumn\u001B[0;34m(self, colName, col)\u001B[0m\n\u001B[1;32m   5169\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(col, Column):\n\u001B[1;32m   5170\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m   5171\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_COLUMN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   5172\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(col)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[1;32m   5173\u001B[0m     )\n\u001B[0;32m-> 5174\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcolName\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcol\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jc\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    181\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    183\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    184\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 185\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    187\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `encryption_key` cannot be resolved. Did you mean one of the following? [`data`, `timestamp`].;\n'Project [data#256, timestamp#244, extract_key('encryption_key)#259 AS encryption_key#260]\n+- Project [from_csv(StructField(order_id,IntegerType,true), StructField(encrypted_account_number,StringType,true), StructField(product,StringType,true), StructField(product_category,StringType,true), StructField(encryption_key,StringType,true), value#253, Some(Asia/Kathmandu), None) AS data#256, timestamp#244]\n   +- Project [cast(value#240 as string) AS value#253, timestamp#244]\n      +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@32028d47, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7ba8ed23, [kafka.bootstrap.servers=localhost:9092, startingOffsets=earliest, subscribe=FC_Account_Master], [key#239, value#240, topic#241, partition#242, offset#243L, timestamp#244, timestampType#245], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@aeb221,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> localhost:9092, subscribe -> FC_Account_Master, startingOffsets -> earliest),None), kafka, [key#232, value#233, topic#234, partition#235, offset#236L, timestamp#237, timestampType#238]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T06:59:58.684103Z",
     "start_time": "2024-05-13T06:59:58.534993Z"
    }
   },
   "source": [
    "df = spark.sql(\"SELECT * FROM testedTable\")\n",
    "df.show(3)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------+----------------+--------------------+\n",
      "|order_id|     account_number|product|product_category|           timestamp|\n",
      "+--------+-------------------+-------+----------------+--------------------+\n",
      "|       0|02XYZXYZ10015592101|    SBA|           SBPPS|2024-05-13 12:14:...|\n",
      "|       1|02XYZXYZ10015593701|    SBA|           SBPPS|2024-05-13 12:14:...|\n",
      "|       2|02XYZXYZ10015593801|    SBA|           SBPPS|2024-05-13 12:14:...|\n",
      "+--------+-------------------+-------+----------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T06:34:56.010837Z",
     "start_time": "2024-05-13T06:34:55.952193Z"
    }
   },
   "source": [
    "df_count = df.count()\n",
    "df_count"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T06:34:53.166125Z",
     "start_time": "2024-05-13T06:34:53.096021Z"
    }
   },
   "source": [
    "df_count = df.count()\n",
    "df_count"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
