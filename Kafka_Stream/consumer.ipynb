{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T05:36:10.610420Z",
     "start_time": "2024-05-14T05:36:05.768011Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "# kafka_topic_name1 = \"FC_Transaction_Base\"\n",
    "kafka_topic_name = 'FC_Account_Master3'\n",
    "kafka_bootstrap_servers = 'localhost:9092'\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Structured Streaming \") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "df = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "        .option(\"subscribe\", kafka_topic_name) \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .load()\n",
    "# fd = spark \\\n",
    "#         .readStream \\\n",
    "#         .format(\"kafka\") \\\n",
    "#         .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "#         .option(\"subscribe\", kafka_topic_name1) \\\n",
    "#         .option(\"startingOffsets\", \"earliest\") \\\n",
    "#         .load()\n",
    "\n",
    "df1 = df.selectExpr(\"CAST(value AS STRING)\", \"timestamp\")\n",
    "# fd1 = fd.selectExpr(\"CAST(value AS STRING)\", \"timestamp\")\n",
    "\n",
    "\n",
    "df_schema_string = \"encrypted_account_number STRING, product STRING, product_category STRING\"\n",
    "# fd_schema_string = \"order_id INT, account_number STRING, branch STRING, transaction_code STRING\"\n",
    "\n",
    "\n",
    "df2 = df1 \\\n",
    "        .select(from_csv(col(\"value\"), df_schema_string) \\\n",
    "                .alias(\"data\"), \"timestamp\")\n",
    "# fd2 = fd1 \\\n",
    "#         .select(from_csv(col(\"value\"), fd_schema_string) \\\n",
    "#                 .alias(\"data1\"), \"timestamp\")\n",
    "\n",
    "\n",
    "df3 = df2.select(\"data.*\", \"timestamp\")\n",
    "\n",
    "# fd3 = fd2.select(\"data1.*\", \"timestamp\")\n",
    "df4 = df3.withColumn('account_number',\n",
    "                               expr(\"trim(aes_decrypt(unbase64(encrypted_account_number), '1234567890abcdef', 'ECB', 'PKCS'))\")).select('account_number','product','product_category')\n",
    "\n",
    "    \n",
    "df4.createOrReplaceTempView(\"data_find\");\n",
    "song_find_text = spark.sql(\"SELECT * FROM data_find\")\n",
    "data_agg_write_stream = song_find_text \\\n",
    "        .writeStream \\\n",
    "        .trigger(processingTime='5 seconds') \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .option(\"truncate\", \"false\") \\\n",
    "        .format(\"memory\") \\\n",
    "        .queryName(\"test5Table\") \\\n",
    "        .start()\n",
    "\n",
    "# fd3.createOrReplaceTempView(\"data1_find\");\n",
    "# song1_find_text = spark.sql(\"SELECT * FROM data1_find\")\n",
    "# data1_agg_write_stream = song1_find_text \\\n",
    "#         .writeStream \\\n",
    "#         .trigger(processingTime='5 seconds') \\\n",
    "#         .outputMode(\"append\") \\\n",
    "#         .option(\"truncate\", \"false\") \\\n",
    "#         .format(\"memory\") \\\n",
    "#         .queryName(\"test2Table\") \\\n",
    "#         .start()\n",
    "\n",
    "data_agg_write_stream.awaitTermination(1)\n",
    "# data1_agg_write_stream.awaitTermination(1)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/14 11:21:06 WARN Utils: Your hostname, Bisheshs-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.13.163.225 instead (on interface en0)\n",
      "24/05/14 11:21:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/bses/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/bses/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-14fbaa50-a580-4285-b136-e6df64c0a149;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 234ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-14fbaa50-a580-4285-b136-e6df64c0a149\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/7ms)\n",
      "24/05/14 11:21:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/14 11:21:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/05/14 11:21:07 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T05:36:15.720643Z",
     "start_time": "2024-05-14T05:36:15.613787Z"
    }
   },
   "source": [
    "df = spark.sql(\"SELECT * FROM test5Table\")\n",
    "df.show(3)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+----------------+\n",
      "|     account_number|product|product_category|\n",
      "+-------------------+-------+----------------+\n",
      "|02XYZXYZ10015592101|    SBA|           SBPPS|\n",
      "|02XYZXYZ10015593701|    SBA|           SBPPS|\n",
      "|02XYZXYZ10015593801|    SBA|           SBPPS|\n",
      "+-------------------+-------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T05:36:38.905925Z",
     "start_time": "2024-05-14T05:36:38.710279Z"
    }
   },
   "source": [
    "df_count = df.count()\n",
    "df_count"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T07:19:06.967347Z",
     "start_time": "2024-05-13T07:19:06.934980Z"
    }
   },
   "source": [
    "fd = spark.sql(\"SELECT * FROM test2Table\")\n",
    "fd.show(3)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+------+----------------+--------------------+\n",
      "|order_id|     account_number|branch|transaction_code|           timestamp|\n",
      "+--------+-------------------+------+----------------+--------------------+\n",
      "|       0|02XYZXYZ10017529992|    15|              CI|2024-05-13 12:44:...|\n",
      "|       1|02XYZXYZ10017529992|    15|              CI|2024-05-13 12:44:...|\n",
      "|       2|02XYZXYZ10017517823|    15|              CI|2024-05-13 12:44:...|\n",
      "+--------+-------------------+------+----------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T07:19:07.546450Z",
     "start_time": "2024-05-13T07:19:07.494447Z"
    }
   },
   "source": [
    "fd_count = fd.count()\n",
    "fd_count"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1163"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
