{
 "cells": [
  {
   "cell_type": "code",
   "id": "0d84e04f-33d9-4852-bb06-80a803ccdfad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:24:11.364174Z",
     "start_time": "2024-05-05T11:24:11.360408Z"
    }
   },
   "source": [
    "import os\n",
    "import warnings\n",
    "import findspark"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "64c2c2d5-19ef-4ec8-b2e0-ec0a059748c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:24:11.747780Z",
     "start_time": "2024-05-05T11:24:11.649841Z"
    }
   },
   "source": [
    "# os.environ['SPARK_HOME'] = \"/Users/bses/spark\"\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = \"jupyter\"\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = \"notebook\"\n",
    "# os.environ['PYSPARK_PYTHON'] = \"python3\"\n",
    "findspark.init()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "1835a3a4-1829-4ec8-b273-bf163273c731",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:24:12.051815Z",
     "start_time": "2024-05-05T11:24:12.050082Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import desc"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "e4687e55-a3be-484e-bc68-c9cef46bc2a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:24:14.277207Z",
     "start_time": "2024-05-05T11:24:12.411850Z"
    }
   },
   "source": [
    "import os\n",
    "import warnings\n",
    "spark = SparkSession.builder \\\n",
    "        .appName('RDD-demo') \\\n",
    "        .getOrCreate()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/05 17:09:13 WARN Utils: Your hostname, Bisheshs-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.13.163.225 instead (on interface en0)\n",
      "24/05/05 17:09:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/05 17:09:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "ddcad5f3-3cae-4c41-928b-2a61e23341a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:24:21.161998Z",
     "start_time": "2024-05-05T11:24:21.027530Z"
    }
   },
   "source": [
    "numbers = [1,2,3,4,5]\n",
    "rdd = spark.sparkContext.parallelize(numbers)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "fb4e41ec-a7f8-4285-be96-64c971a53071",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:24:22.429073Z",
     "start_time": "2024-05-05T11:24:21.953233Z"
    }
   },
   "source": "rdd.collect()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:24:22.433602Z",
     "start_time": "2024-05-05T11:24:22.430105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = [('A',25),('B',25),('A',30),('A',40)]\n",
    "rdd1 = spark.sparkContext.parallelize(data)"
   ],
   "id": "ff4425e139e04ca5",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:24:22.962207Z",
     "start_time": "2024-05-05T11:24:22.918417Z"
    }
   },
   "cell_type": "code",
   "source": "rdd1.collect()",
   "id": "d4c2afdace0f6cb7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 25), ('B', 25), ('A', 30), ('A', 40)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Actions",
   "id": "593d834698855737"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:24:24.459547Z",
     "start_time": "2024-05-05T11:24:23.925735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "count = rdd.count()\n",
    "print('Count:', count)"
   ],
   "id": "b846e28612fc9c1d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 5\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:24:24.579792Z",
     "start_time": "2024-05-05T11:24:24.460632Z"
    }
   },
   "cell_type": "code",
   "source": "print('First item:', rdd.first())",
   "id": "8cb768a722129582",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First item: 1\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:24:24.770645Z",
     "start_time": "2024-05-05T11:24:24.650364Z"
    }
   },
   "cell_type": "code",
   "source": "print('First 3 elements:', rdd.take(3))",
   "id": "3c73a28fdc460685",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 elements: [1, 2, 3]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:24:25.139971Z",
     "start_time": "2024-05-05T11:24:25.033863Z"
    }
   },
   "cell_type": "code",
   "source": "rdd.foreach(lambda x: print(x**2))",
   "id": "e2b6c67761adf036",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1\n",
      "16\n",
      "4\n",
      "9\n",
      "25\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Transformation",
   "id": "705386f9e0c85a4e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:24:26.028430Z",
     "start_time": "2024-05-05T11:24:26.026233Z"
    }
   },
   "cell_type": "code",
   "source": "mapped_rdd = rdd.map(lambda x: (x**2, x**3))",
   "id": "3176cc1b0ab06f60",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:24:26.576174Z",
     "start_time": "2024-05-05T11:24:26.491129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = mapped_rdd.collect()\n",
    "print('Mapped rdd:', result)"
   ],
   "id": "16021945aac373b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped rdd: [(1, 1), (4, 8), (9, 27), (16, 64), (25, 125)]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:22:46.919668Z",
     "start_time": "2024-05-05T11:22:46.826648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filtered = rdd.filter(lambda x: (x**2) < 17)\n",
    "filtered.collect()"
   ],
   "id": "aa4bb820e0ab4fbe",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:22:47.970355Z",
     "start_time": "2024-05-05T11:22:47.605056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "reduced_rdd = rdd1.reduceByKey(lambda x,y: x+y)\n",
    "reduced_rdd.collect()"
   ],
   "id": "510359cb5ec8942b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', 25), ('A', 95)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:22:48.314480Z",
     "start_time": "2024-05-05T11:22:47.971287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sorted_rdd = rdd1.sortBy(lambda x: (x[0], x[1]))\n",
    "sorted_rdd.collect()"
   ],
   "id": "d045fe8e97aed6bc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 25), ('A', 30), ('A', 40), ('B', 25)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:22:48.621347Z",
     "start_time": "2024-05-05T11:22:48.369402Z"
    }
   },
   "cell_type": "code",
   "source": "rdd.saveAsTextFile('rdd.txt')",
   "id": "ea0c7b8a8c056f2",
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o200.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/Users/bses/F1_Intern/Spark_tuto/rdd.txt already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:299)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1623)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1623)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1609)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1609)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:564)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:563)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mrdd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msaveAsTextFile\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrdd.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/spark/python/pyspark/rdd.py:3425\u001B[0m, in \u001B[0;36mRDD.saveAsTextFile\u001B[0;34m(self, path, compressionCodecClass)\u001B[0m\n\u001B[1;32m   3423\u001B[0m     keyed\u001B[38;5;241m.\u001B[39m_jrdd\u001B[38;5;241m.\u001B[39mmap(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mctx\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mBytesToString())\u001B[38;5;241m.\u001B[39msaveAsTextFile(path, compressionCodec)\n\u001B[1;32m   3424\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 3425\u001B[0m     \u001B[43mkeyed\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jrdd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mBytesToString\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msaveAsTextFile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m~/spark/python/pyspark/errors/exceptions/captured.py:179\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    178\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 179\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    180\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    181\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
      "File \u001B[0;32m~/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o200.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/Users/bses/F1_Intern/Spark_tuto/rdd.txt already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:299)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1623)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1623)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1609)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1609)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:564)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:563)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:22:49.043839Z",
     "start_time": "2024-05-05T11:22:48.887726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rdd_text = spark.sparkContext.textFile('rdd.txt')\n",
    "rdd_text.collect()"
   ],
   "id": "137b2a790834ce8a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2', '3', '1', '4', '5']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T10:49:59.153480Z",
     "start_time": "2024-05-05T10:49:58.525481Z"
    }
   },
   "cell_type": "code",
   "source": "spark.stop()",
   "id": "703e055ce20e8bfd",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataframe",
   "id": "490741acde3a27a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:22:52.409206Z",
     "start_time": "2024-05-05T11:22:52.208717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rdd = spark.sparkContext.textFile('./data/data.txt')\n",
    "result_rdd = rdd.flatMap(lambda line: line.split(\" \")).map(lambda word:(word, 1)).reduceByKey(lambda x, y: x+y).sortBy(lambda x: x[1], ascending=False)"
   ],
   "id": "63fd4d911fd4f23f",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:22:52.797289Z",
     "start_time": "2024-05-05T11:22:52.649336Z"
    }
   },
   "cell_type": "code",
   "source": "result_rdd.take(30)",
   "id": "4d01c41263ac6731",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 12),\n",
       " ('of', 7),\n",
       " ('a', 7),\n",
       " ('distributed', 5),\n",
       " ('in', 5),\n",
       " ('Spark', 4),\n",
       " ('as', 3),\n",
       " ('is', 3),\n",
       " ('API', 3),\n",
       " ('on', 3),\n",
       " ('Dataset', 3),\n",
       " ('RDD', 3),\n",
       " ('and', 2),\n",
       " ('results', 2),\n",
       " ('its', 2),\n",
       " ('data', 2),\n",
       " ('cluster', 2),\n",
       " ('that', 2),\n",
       " ('The', 2),\n",
       " ('was', 2),\n",
       " ('API.', 2),\n",
       " ('RDDs', 2),\n",
       " ('MapReduce', 2),\n",
       " ('programs', 2),\n",
       " ('function', 2),\n",
       " ('resilient', 1),\n",
       " ('dataset', 1),\n",
       " ('read-only', 1),\n",
       " ('multiset', 1),\n",
       " ('machines,', 1)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:22:53.866620Z",
     "start_time": "2024-05-05T11:22:53.339919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = spark.read.text('./data/data.txt')\n",
    "\n",
    "result_df = df.selectExpr('explode(split(value, \" \")) as word').groupby('word').count().orderBy(desc('count'))"
   ],
   "id": "e0a7e5fd98fc719b",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:22:54.552561Z",
     "start_time": "2024-05-05T11:22:53.867734Z"
    }
   },
   "cell_type": "code",
   "source": "result_df.take(10)",
   "id": "7e1c12211e077e5d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(word='the', count=12),\n",
       " Row(word='of', count=7),\n",
       " Row(word='a', count=7),\n",
       " Row(word='in', count=5),\n",
       " Row(word='distributed', count=5),\n",
       " Row(word='Spark', count=4),\n",
       " Row(word='API', count=3),\n",
       " Row(word='RDD', count=3),\n",
       " Row(word='is', count=3),\n",
       " Row(word='on', count=3)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:22:55.161175Z",
     "start_time": "2024-05-05T11:22:54.553610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "csv_file_path = './data/products.csv'\n",
    "df = spark.read.csv(csv_file_path, header=True)"
   ],
   "id": "c5e2ab943e486acf",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:22:55.256093Z",
     "start_time": "2024-05-05T11:22:55.161953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df.printSchema()\n",
    "df.show(5)"
   ],
   "id": "f24f7306b0bb88ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantity: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      "\n",
      "+---+--------------------+---------------+--------+------+\n",
      "| id|                name|       category|quantity| price|\n",
      "+---+--------------------+---------------+--------+------+\n",
      "|  1|           iPhone 12|    Electronics|      10|899.99|\n",
      "|  2|     Nike Air Max 90|       Clothing|      25|119.99|\n",
      "|  3|KitchenAid Stand ...|Home Appliances|       5|299.99|\n",
      "|  4|    The Great Gatsby|          Books|      50| 12.99|\n",
      "|  5|L'Oreal Paris Mas...|         Beauty|     100|  9.99|\n",
      "+---+--------------------+---------------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:22:55.259465Z",
     "start_time": "2024-05-05T11:22:55.257483Z"
    }
   },
   "cell_type": "code",
   "source": "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType",
   "id": "21a316a216849b73",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:22:55.261914Z",
     "start_time": "2024-05-05T11:22:55.260089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "schema = StructType([\n",
    "        StructField(name = 'id', dataType = IntegerType(), nullable = True),\n",
    "        StructField(name = 'name', dataType = StringType(), nullable = True),\n",
    "        StructField(name = 'category', dataType = IntegerType(), nullable = True),\n",
    "        StructField(name = 'quantity', dataType = StringType(), nullable = True),\n",
    "        StructField(name = 'price', dataType = DoubleType(), nullable = True),\n",
    "])"
   ],
   "id": "4f71ef2e4ffc13aa",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:22:55.277891Z",
     "start_time": "2024-05-05T11:22:55.262502Z"
    }
   },
   "cell_type": "code",
   "source": "df = spark.read.csv('./data/products.csv', header=True, schema=schema)",
   "id": "ee55b63167d646a6",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:22:55.590032Z",
     "start_time": "2024-05-05T11:22:55.525222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df.printSchema()\n",
    "df.show(5)"
   ],
   "id": "4edf33a9874a2a72",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- category: integer (nullable = true)\n",
      " |-- quantity: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n",
      "+---+--------------------+--------+--------+------+\n",
      "| id|                name|category|quantity| price|\n",
      "+---+--------------------+--------+--------+------+\n",
      "|  1|           iPhone 12|    NULL|      10|899.99|\n",
      "|  2|     Nike Air Max 90|    NULL|      25|119.99|\n",
      "|  3|KitchenAid Stand ...|    NULL|       5|299.99|\n",
      "|  4|    The Great Gatsby|    NULL|      50| 12.99|\n",
      "|  5|L'Oreal Paris Mas...|    NULL|     100|  9.99|\n",
      "+---+--------------------+--------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:22:55.930268Z",
     "start_time": "2024-05-05T11:22:55.821860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = spark.read.csv('./data/products.csv', header=True, inferSchema=True)\n",
    "\n",
    "df.printSchema()"
   ],
   "id": "374eaaca879f60ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:22:56.267015Z",
     "start_time": "2024-05-05T11:22:56.113005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = spark.read.csv('./data/stocks.txt', header=True, inferSchema=True)\n",
    "df.printSchema()\n",
    "df.show(5)"
   ],
   "id": "a0d0de7f40187273",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n",
      "+---+----------+-----------+--------+-------+\n",
      "| id|      name|   category|quantity|  price|\n",
      "+---+----------+-----------+--------+-------+\n",
      "|  1|    iPhone|Electronics|      10| 899.99|\n",
      "|  2|   Macbook|Electronics|       5|1299.99|\n",
      "|  3|      iPad|Electronics|      15| 499.99|\n",
      "|  4|Samsung TV|Electronics|       8| 799.99|\n",
      "|  5|     LG TV|Electronics|      10| 699.99|\n",
      "+---+----------+-----------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:22:56.483403Z",
     "start_time": "2024-05-05T11:22:56.421305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df1 = df.select(\"id\", \"name\", \"category\", \"price\")\n",
    "df1.show(5)"
   ],
   "id": "22181f108a88daf4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+-------+\n",
      "| id|      name|   category|  price|\n",
      "+---+----------+-----------+-------+\n",
      "|  1|    iPhone|Electronics| 899.99|\n",
      "|  2|   Macbook|Electronics|1299.99|\n",
      "|  3|      iPad|Electronics| 499.99|\n",
      "|  4|Samsung TV|Electronics| 799.99|\n",
      "|  5|     LG TV|Electronics| 699.99|\n",
      "+---+----------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:22:56.818967Z",
     "start_time": "2024-05-05T11:22:56.754148Z"
    }
   },
   "cell_type": "code",
   "source": "df.filter(df.quantity>20).show(5)",
   "id": "7c923a1cf91d165f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------+--------+-----+\n",
      "| id|        name|category|quantity|price|\n",
      "+---+------------+--------+--------+-----+\n",
      "|  6|  Nike Shoes|Clothing|      30|99.99|\n",
      "|  7|Adidas Shoes|Clothing|      25|89.99|\n",
      "| 12|      Apples|    Food|     100|  0.5|\n",
      "| 13|     Bananas|    Food|     150| 0.25|\n",
      "| 14|     Oranges|    Food|     120| 0.75|\n",
      "+---+------------+--------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:22:57.268385Z",
     "start_time": "2024-05-05T11:22:57.098477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "grouped_data = df.groupBy('category').agg({'quantity':'sum', 'price':'avg'})\n",
    "grouped_data.show(5)"
   ],
   "id": "8628c5695770f141",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------------------+\n",
      "|   category|sum(quantity)|        avg(price)|\n",
      "+-----------+-------------+------------------+\n",
      "|       Food|          450|2.2960000000000003|\n",
      "|     Sports|           35|             34.99|\n",
      "|Electronics|           98| 586.6566666666665|\n",
      "|   Clothing|          200|  99.2757142857143|\n",
      "|  Furniture|           41|            141.99|\n",
      "+-----------+-------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:22:57.645143Z",
     "start_time": "2024-05-05T11:22:57.439792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df2 = df.select('id','category').limit(10)\n",
    "joined_df = df.join(df2, \"id\",\"inner\")\n",
    "joined_df.show(5)"
   ],
   "id": "85994c2f0faebd17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+--------+-------+-----------+\n",
      "| id|      name|   category|quantity|  price|   category|\n",
      "+---+----------+-----------+--------+-------+-----------+\n",
      "|  1|    iPhone|Electronics|      10| 899.99|Electronics|\n",
      "|  2|   Macbook|Electronics|       5|1299.99|Electronics|\n",
      "|  3|      iPad|Electronics|      15| 499.99|Electronics|\n",
      "|  4|Samsung TV|Electronics|       8| 799.99|Electronics|\n",
      "|  5|     LG TV|Electronics|      10| 699.99|Electronics|\n",
      "+---+----------+-----------+--------+-------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:22:57.826771Z",
     "start_time": "2024-05-05T11:22:57.763457Z"
    }
   },
   "cell_type": "code",
   "source": "df.orderBy('price').show(5)",
   "id": "e36e71c467fe04f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+--------+--------+-----+\n",
      "| id|          name|category|quantity|price|\n",
      "+---+--------------+--------+--------+-----+\n",
      "| 13|       Bananas|    Food|     150| 0.25|\n",
      "| 12|        Apples|    Food|     100|  0.5|\n",
      "| 14|       Oranges|    Food|     120| 0.75|\n",
      "| 15|Chicken Breast|    Food|      50| 3.99|\n",
      "| 16| Salmon Fillet|    Food|      30| 5.99|\n",
      "+---+--------------+--------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:22:58.156400Z",
     "start_time": "2024-05-05T11:22:58.087207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.orderBy(col('price').desc(), col('id').desc()).show(5)"
   ],
   "id": "cb20fe40dae114b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+--------+-------+\n",
      "| id|      name|   category|quantity|  price|\n",
      "+---+----------+-----------+--------+-------+\n",
      "|  2|   Macbook|Electronics|       5|1299.99|\n",
      "|  1|    iPhone|Electronics|      10| 899.99|\n",
      "|  4|Samsung TV|Electronics|       8| 799.99|\n",
      "|  5|     LG TV|Electronics|      10| 699.99|\n",
      "| 26|    Camera|Electronics|      10| 599.99|\n",
      "+---+----------+-----------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:22:58.531935Z",
     "start_time": "2024-05-05T11:22:58.426857Z"
    }
   },
   "cell_type": "code",
   "source": "df.select('category').distinct().show(5)",
   "id": "20e971d69918669c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|   category|\n",
      "+-----------+\n",
      "|       Food|\n",
      "|     Sports|\n",
      "|Electronics|\n",
      "|   Clothing|\n",
      "|  Furniture|\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:22:58.761437Z",
     "start_time": "2024-05-05T11:22:58.717737Z"
    }
   },
   "cell_type": "code",
   "source": "df.drop(\"quantity\",\"category\").show(10)",
   "id": "74cb9fdf75f5951f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-------+\n",
      "| id|            name|  price|\n",
      "+---+----------------+-------+\n",
      "|  1|          iPhone| 899.99|\n",
      "|  2|         Macbook|1299.99|\n",
      "|  3|            iPad| 499.99|\n",
      "|  4|      Samsung TV| 799.99|\n",
      "|  5|           LG TV| 699.99|\n",
      "|  6|      Nike Shoes|  99.99|\n",
      "|  7|    Adidas Shoes|  89.99|\n",
      "|  8| Sony Headphones| 149.99|\n",
      "|  9|Beats Headphones| 199.99|\n",
      "| 10|    Dining Table| 249.99|\n",
      "+---+----------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:22:59.132544Z",
     "start_time": "2024-05-05T11:22:59.065560Z"
    }
   },
   "cell_type": "code",
   "source": "df.withColumn('revenue', df.price * df.quantity).show(5)",
   "id": "1af4a2a8534e0868",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+--------+-------+-------+\n",
      "| id|      name|   category|quantity|  price|revenue|\n",
      "+---+----------+-----------+--------+-------+-------+\n",
      "|  1|    iPhone|Electronics|      10| 899.99| 8999.9|\n",
      "|  2|   Macbook|Electronics|       5|1299.99|6499.95|\n",
      "|  3|      iPad|Electronics|      15| 499.99|7499.85|\n",
      "|  4|Samsung TV|Electronics|       8| 799.99|6399.92|\n",
      "|  5|     LG TV|Electronics|      10| 699.99| 6999.9|\n",
      "+---+----------+-----------+--------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T11:22:59.655719Z",
     "start_time": "2024-05-05T11:22:59.615767Z"
    }
   },
   "cell_type": "code",
   "source": "df_with_alias = df.withColumnRenamed('price','product_price').show(5)",
   "id": "1a9fd360d11be4ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+--------+-------------+\n",
      "| id|      name|   category|quantity|product_price|\n",
      "+---+----------+-----------+--------+-------------+\n",
      "|  1|    iPhone|Electronics|      10|       899.99|\n",
      "|  2|   Macbook|Electronics|       5|      1299.99|\n",
      "|  3|      iPad|Electronics|      15|       499.99|\n",
      "|  4|Samsung TV|Electronics|       8|       799.99|\n",
      "|  5|     LG TV|Electronics|      10|       699.99|\n",
      "+---+----------+-----------+--------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "24524e8c63ca0b7c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
